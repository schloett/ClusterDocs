{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IKIM Scientific Computing This website contains documentation for the scientific computing infrastructure at the Institute for AI in Medicine (IKIM) in Essen. The documentation is geared towards researchers and students that aim to run scientific experiments on the cluster. See Getting Started for general instructions. About The sources of this documentation can be found on GitHub and we encourage contribution.","title":"Introduction"},{"location":"#ikim-scientific-computing","text":"This website contains documentation for the scientific computing infrastructure at the Institute for AI in Medicine (IKIM) in Essen. The documentation is geared towards researchers and students that aim to run scientific experiments on the cluster. See Getting Started for general instructions.","title":"IKIM Scientific Computing"},{"location":"#about","text":"The sources of this documentation can be found on GitHub and we encourage contribution.","title":"About"},{"location":"conda/","text":"","title":"Conda and Mamba"},{"location":"faq/","text":"Frequently Asked Questions VSCode fails set itself up on a remote host Enable the VSCode setting remote.SSH.lockfilesInTmp . A program fails with the message too many open files This limit can be changed for the current user session with ulimit -Sn followed by the desired number. For example: ulimit -Sn 4096 It is advisable to execute the ulimit command only when a workflow requires keeping many files open at the same time. For example: ulimit -Sn 4096 && python mywork.py Fixing permissions for shared files If the permissions of files that are supposed to be shared are too restrictive, ask the owner to extend the permissions with: # Allow everyone to read everything in a directory. chmod -R a+r <path to directory> # Allow the owner's group to modify every file in a directory. chmod -R g+w <path to directory> # Extend the execution and directory browsing permissions that the owner has in a directory to everyone. find <path to directory> -executable -exec chmod a+x {} \\;","title":"FAQs"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#vscode-fails-set-itself-up-on-a-remote-host","text":"Enable the VSCode setting remote.SSH.lockfilesInTmp .","title":"VSCode fails set itself up on a remote host"},{"location":"faq/#a-program-fails-with-the-message-too-many-open-files","text":"This limit can be changed for the current user session with ulimit -Sn followed by the desired number. For example: ulimit -Sn 4096 It is advisable to execute the ulimit command only when a workflow requires keeping many files open at the same time. For example: ulimit -Sn 4096 && python mywork.py","title":"A program fails with the message too many open files"},{"location":"faq/#fixing-permissions-for-shared-files","text":"If the permissions of files that are supposed to be shared are too restrictive, ask the owner to extend the permissions with: # Allow everyone to read everything in a directory. chmod -R a+r <path to directory> # Allow the owner's group to modify every file in a directory. chmod -R g+w <path to directory> # Extend the execution and directory browsing permissions that the owner has in a directory to everyone. find <path to directory> -executable -exec chmod a+x {} \\;","title":"Fixing permissions for shared files"},{"location":"getting-started/","text":"Getting Started Welcome the IKIM cluster documentation. The goal of this document is to give you enough background to work on the IKIM cluster. It is not meant as a general introduction to remote computing services. We will refer to external sources where necessary. If you have any questions, please reach out to your project coordinator for help. Getting cluster access Note: we assume that you are using a Linux or MacOS operating system. If you are using Windows, please see below for recommendations. To get access to the IKIM computing infrastructure you need an SSH key. Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager. ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_ikim Please send the public key along with following contact details to your project coordinator: Desired username (e.g., firstname.lastname ) First name Last name UK/UDE email Public SSH key ( ~/.ssh/id_ikim.pub ) Afterwards, an account will be created for you in the central user management. When this is done, you should be able to SSH into the cluster. Example: output of SSH-keypair generation. When executing the command above, you should should see output similar to this: $ ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/Users/<user>/.ssh/id_ikim): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/<user>/.ssh/id_ikim. Your public key has been saved in /Users/<user>/.ssh/id_ikim.pub. The key fingerprint is: SHA256:Fq6OklSiCUQ3G1UsnDu8dFb+VwBfMakRGROe+A2D78Y user@<host> The key's randomart image is: +---[RSA 4096]----+ |.. +o.+. ..==+o| | .. ++ . . =++..| |. .. o + o *+ | |. . .= + o o.+. | |.o o. = S . o.. | |o . . o .o. | | . . . .E | | o o . | | .. . | +----[SHA256]-----+ Note that two files were created in your home directory in the `.ssh` subdirectory: $ ls ~/.ssh config id_ikim id_ikim.pub known_hosts - `~/.ssh/id_ikim` - This is your private SSH key. Treat this file like a password. Do not share it with anyone. - `~/.ssh/id_ikim.pub` - This is your public SSH key. This should be shared with your project coordinator. You can open it with any text editor. The contents of `~/.ssh/id_ikim.pub` look similar to this: $ cat ~/.ssh/id_ikim.pub ssh-rsa [ very long random string ]== <user>@<host> Setting up your SSH configuration There is only a single gateway into the IKIM cluster which is called the jumphost. This host should only be used to connect to other servers on the cluster and should not be used for computational work. Use the command below to login to any host, replacing $USERNAME and $HOSTNAME appropriately. ssh -J $USERNAME @login.ikim.uk-essen.de $USERNAME @ $HOSTNAME # Example ssh -J john.doe@login.ikim.uk-essen.de john.doe@c52 For convenience, you can configure your SSH client to automatically use the jumphost. Place the snippet below into your ~/.ssh/config file, replacing $USERNAME appropriately. Host * AddKeysToAgent yes IdentityFile ~/.ssh/id_ikim CanonicalizeHostname yes Host ikim HostName login.ikim.uk-essen.de User $USERNAME ForwardAgent yes Host g1-? c? c?? Hostname %h.ikim.uk-essen.de ForwardAgent yes Host g1-*.ikim.uk-essen.de c*.ikim.uk-essen.de User $USERNAME ProxyJump ikim ForwardAgent yes Test your SSH login Try some of the examples below to test that your SSH client is properly configured: # Login to CPU-node c52 ssh c52.ikim.uk-essen.de # Shorthand for above command ssh c52 # Login to GPU-node ssh g1-7 If any of the above is not working, please run the command below and send the debug message to your project coordinator for help. ssh -v $USERNAME @login.ikim.uk-essen.de SSH clients on Windows We recommend two options for installing and using an SSH client on Windows: Windows Subsystem for Linux (WSL2) provides Linux distributions running in a lightweight virtual machine on Windows. With WSL, the instructions above can be followed without changes and the default shell environment is identical to the one found on IKIM hosts. OpenSSH is the same software suite that comes preinstalled on other operating systems. To install it, go to the Apps & Features settings page and select Optional Features , then add the OpenSSH Client feature. The instructions above should work simply by adapting paths to Windows-style. Older clients might produce an error message that starts with \"Bad stdio forwarding specification\" , which can be fixed by replacing the ProxyJump directive with: ProxyCommand ssh.exe -W %h:%p ikim What hardware is available on the IKIM cluster? The cluster has two sets of servers: 120 nodes for CPU-bound tasks and 10 nodes for GPU-bound tasks. At this moment, not all of these nodes are available for general computation tasks. However, more will be added in future. The following hardware is installed in the servers: CPU nodes ( c_nodes ): Each with 192GB RAM, 2 CPU Intel, 1 SSD for system and 1 SSD for data (2TB). GPU nodes ( g_nodes ): Each with 6 NVIDIA RTX 6000 GPUs, 1024GB RAM, 2 CPU AMD, 1 SSD for system (1TB) and 2 NWMe for data (12TB configured as RAID-0). Your project coordinator can give you recommendations on where to run your jobs. What software is available on the IKIM cluster? We aim to keep the computing environments on the cluster as clean as possible. Therefore, only commonly used software packages are pre-installed and configured. At this moment this includes: Python 3 Conda Docker (for applications with dependencies not available in the default set of software). We assume basic familiarity with the tools above. If you want to learn more, you can take a look at following resources: Docker Curriculum Conda Getting Started Where to store your data? There are several locations where you can store data on the cluster: Your home directory ( /homes/<username>/ ): This directory is only for personal data such as configuration files. Anything related to work or that should be visible to other people should not reside here. Project pirectory ( /projects/<project_name>/ ): This location should be used for data related to your project. If you are starting a project, ask your project coordinator to create a directory and provide a list of participating users. Note that you cannot simply list all project directories via ls /projects ; instead, you need to specify the full path, such as: ls /projects/dso_mp_ws2021/ Public dataset directory ( /projects/datashare ): A world-readable location for datasets for which no special access rights are required. To lower the risk of data loss, each user can write only in a subdirectory corresponding to their research group. For example, a user which belongs to group tio must add new datasets in /projects/datashare/tio but can browse and read throughout /projects/datashare . Group directory ( /groups/<group_name> ): This is the appropriate place for any data that should be shared within an IKIM research group . In student projects you will most likely not need group directories. All of the above directories (homes, projects, groups) are shared with other hosts on the cluster through the network file system (NFS). This is convenient: sharing data between hosts becomes effortless and your data is stored redundantly on the file server. Local-only files For some operations the NFS comes with unnecessary overhead. Therefore, the path /local/work is available for creating files and directories that reside on the storage drive of the current host. This location should only be used for quick testing, preliminary experimentation and intermediate output. As soon as you need your files saved, move them to /projects or /groups . Local-only files are not backed up and can be deleted without notice . Here are tips on writing programs, scripts, containers, etc. that make good use of network resources: Read inputs from and write the final results to /projects or /groups . Write intermediate output to /local/work . NFS caching Read operations on remote files (homes, projects, groups) are cached transparently on the local storage drive. Generally speaking, your first access to a dataset will be limited by network bandwidth, but any subsequent access will be made from local storage to should be substantially faster. Tips on Working with remote computing services Unix Crash Course Another Unix Course Tactical tmux: The 10 Most Important Commands How To Use Linux Screen Git Book Conda","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"Welcome the IKIM cluster documentation. The goal of this document is to give you enough background to work on the IKIM cluster. It is not meant as a general introduction to remote computing services. We will refer to external sources where necessary. If you have any questions, please reach out to your project coordinator for help.","title":"Getting Started"},{"location":"getting-started/#getting-cluster-access","text":"Note: we assume that you are using a Linux or MacOS operating system. If you are using Windows, please see below for recommendations. To get access to the IKIM computing infrastructure you need an SSH key. Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager. ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_ikim Please send the public key along with following contact details to your project coordinator: Desired username (e.g., firstname.lastname ) First name Last name UK/UDE email Public SSH key ( ~/.ssh/id_ikim.pub ) Afterwards, an account will be created for you in the central user management. When this is done, you should be able to SSH into the cluster. Example: output of SSH-keypair generation. When executing the command above, you should should see output similar to this: $ ssh-keygen -t rsa -b 4096 Generating public/private rsa key pair. Enter file in which to save the key (/Users/<user>/.ssh/id_ikim): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /Users/<user>/.ssh/id_ikim. Your public key has been saved in /Users/<user>/.ssh/id_ikim.pub. The key fingerprint is: SHA256:Fq6OklSiCUQ3G1UsnDu8dFb+VwBfMakRGROe+A2D78Y user@<host> The key's randomart image is: +---[RSA 4096]----+ |.. +o.+. ..==+o| | .. ++ . . =++..| |. .. o + o *+ | |. . .= + o o.+. | |.o o. = S . o.. | |o . . o .o. | | . . . .E | | o o . | | .. . | +----[SHA256]-----+ Note that two files were created in your home directory in the `.ssh` subdirectory: $ ls ~/.ssh config id_ikim id_ikim.pub known_hosts - `~/.ssh/id_ikim` - This is your private SSH key. Treat this file like a password. Do not share it with anyone. - `~/.ssh/id_ikim.pub` - This is your public SSH key. This should be shared with your project coordinator. You can open it with any text editor. The contents of `~/.ssh/id_ikim.pub` look similar to this: $ cat ~/.ssh/id_ikim.pub ssh-rsa [ very long random string ]== <user>@<host>","title":"Getting cluster access"},{"location":"getting-started/#setting-up-your-ssh-configuration","text":"There is only a single gateway into the IKIM cluster which is called the jumphost. This host should only be used to connect to other servers on the cluster and should not be used for computational work. Use the command below to login to any host, replacing $USERNAME and $HOSTNAME appropriately. ssh -J $USERNAME @login.ikim.uk-essen.de $USERNAME @ $HOSTNAME # Example ssh -J john.doe@login.ikim.uk-essen.de john.doe@c52 For convenience, you can configure your SSH client to automatically use the jumphost. Place the snippet below into your ~/.ssh/config file, replacing $USERNAME appropriately. Host * AddKeysToAgent yes IdentityFile ~/.ssh/id_ikim CanonicalizeHostname yes Host ikim HostName login.ikim.uk-essen.de User $USERNAME ForwardAgent yes Host g1-? c? c?? Hostname %h.ikim.uk-essen.de ForwardAgent yes Host g1-*.ikim.uk-essen.de c*.ikim.uk-essen.de User $USERNAME ProxyJump ikim ForwardAgent yes","title":"Setting up your SSH configuration"},{"location":"getting-started/#test-your-ssh-login","text":"Try some of the examples below to test that your SSH client is properly configured: # Login to CPU-node c52 ssh c52.ikim.uk-essen.de # Shorthand for above command ssh c52 # Login to GPU-node ssh g1-7 If any of the above is not working, please run the command below and send the debug message to your project coordinator for help. ssh -v $USERNAME @login.ikim.uk-essen.de","title":"Test your SSH login"},{"location":"getting-started/#ssh-clients-on-windows","text":"We recommend two options for installing and using an SSH client on Windows: Windows Subsystem for Linux (WSL2) provides Linux distributions running in a lightweight virtual machine on Windows. With WSL, the instructions above can be followed without changes and the default shell environment is identical to the one found on IKIM hosts. OpenSSH is the same software suite that comes preinstalled on other operating systems. To install it, go to the Apps & Features settings page and select Optional Features , then add the OpenSSH Client feature. The instructions above should work simply by adapting paths to Windows-style. Older clients might produce an error message that starts with \"Bad stdio forwarding specification\" , which can be fixed by replacing the ProxyJump directive with: ProxyCommand ssh.exe -W %h:%p ikim","title":"SSH clients on Windows"},{"location":"getting-started/#what-hardware-is-available-on-the-ikim-cluster","text":"The cluster has two sets of servers: 120 nodes for CPU-bound tasks and 10 nodes for GPU-bound tasks. At this moment, not all of these nodes are available for general computation tasks. However, more will be added in future. The following hardware is installed in the servers: CPU nodes ( c_nodes ): Each with 192GB RAM, 2 CPU Intel, 1 SSD for system and 1 SSD for data (2TB). GPU nodes ( g_nodes ): Each with 6 NVIDIA RTX 6000 GPUs, 1024GB RAM, 2 CPU AMD, 1 SSD for system (1TB) and 2 NWMe for data (12TB configured as RAID-0). Your project coordinator can give you recommendations on where to run your jobs.","title":"What hardware is available on the IKIM cluster?"},{"location":"getting-started/#what-software-is-available-on-the-ikim-cluster","text":"We aim to keep the computing environments on the cluster as clean as possible. Therefore, only commonly used software packages are pre-installed and configured. At this moment this includes: Python 3 Conda Docker (for applications with dependencies not available in the default set of software). We assume basic familiarity with the tools above. If you want to learn more, you can take a look at following resources: Docker Curriculum Conda Getting Started","title":"What software is available on the IKIM cluster?"},{"location":"getting-started/#where-to-store-your-data","text":"There are several locations where you can store data on the cluster: Your home directory ( /homes/<username>/ ): This directory is only for personal data such as configuration files. Anything related to work or that should be visible to other people should not reside here. Project pirectory ( /projects/<project_name>/ ): This location should be used for data related to your project. If you are starting a project, ask your project coordinator to create a directory and provide a list of participating users. Note that you cannot simply list all project directories via ls /projects ; instead, you need to specify the full path, such as: ls /projects/dso_mp_ws2021/ Public dataset directory ( /projects/datashare ): A world-readable location for datasets for which no special access rights are required. To lower the risk of data loss, each user can write only in a subdirectory corresponding to their research group. For example, a user which belongs to group tio must add new datasets in /projects/datashare/tio but can browse and read throughout /projects/datashare . Group directory ( /groups/<group_name> ): This is the appropriate place for any data that should be shared within an IKIM research group . In student projects you will most likely not need group directories. All of the above directories (homes, projects, groups) are shared with other hosts on the cluster through the network file system (NFS). This is convenient: sharing data between hosts becomes effortless and your data is stored redundantly on the file server.","title":"Where to store your data?"},{"location":"getting-started/#local-only-files","text":"For some operations the NFS comes with unnecessary overhead. Therefore, the path /local/work is available for creating files and directories that reside on the storage drive of the current host. This location should only be used for quick testing, preliminary experimentation and intermediate output. As soon as you need your files saved, move them to /projects or /groups . Local-only files are not backed up and can be deleted without notice . Here are tips on writing programs, scripts, containers, etc. that make good use of network resources: Read inputs from and write the final results to /projects or /groups . Write intermediate output to /local/work .","title":"Local-only files"},{"location":"getting-started/#nfs-caching","text":"Read operations on remote files (homes, projects, groups) are cached transparently on the local storage drive. Generally speaking, your first access to a dataset will be limited by network bandwidth, but any subsequent access will be made from local storage to should be substantially faster.","title":"NFS caching"},{"location":"getting-started/#tips-on-working-with-remote-computing-services","text":"Unix Crash Course Another Unix Course Tactical tmux: The 10 Most Important Commands How To Use Linux Screen Git Book Conda","title":"Tips on Working with remote computing services"},{"location":"jupyter/","text":"Jupyter Notebook Workflow The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Visit Jupyter.org for more information. Starting Jupyter on the Server To work with Jupyter notebooks on the cluster, a typical workflow is as follows: First, login to a node of the cluster where you'd like to run Jupyter on. # For example: ssh g1-7 Then, create a conda environment with your project dependencies and Jupyter. conda create -n myproject -c conda-forge python = 3 .8 pandas notebook conda activate myproject Start Jupyter instance on the cluster that listens on the networking interface at port 8888. If you have used Jupyter before, the output of this command should look similar to what you see on your local machine. $ jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser [I 06:30:19.290 NotebookApp] Serving notebooks from local directory: /homes/jan [I 06:30:19.290 NotebookApp] Jupyter Notebook 6.4.5 is running at: [I 06:30:19.290 NotebookApp] http://g1-7.ikim.uk-essen.de:8888/ ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 [I 06:30:19.290 NotebookApp] or http://127.0.0.1:8888/ ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 [I 06:30:19.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 06:30:19.296 NotebookApp] To access the notebook, open this file in a browser: file:///homes/jan/.local/share/jupyter/runtime/nbserver-426528-open.html Or copy and paste one of these URLs: http://g1-7.ikim.uk-essen.de:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 or http://127.0.0.1:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 Pay attention to the token in the output. In this case: d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 . This is the password which will allow you to access to the Jupyter session once you have connected your local machine to the remote Jupyter Session. Connecting to the remote Jupyter Finally, run the following command on your local machine, to setup an SSH tunnel that connects to the remote Jupyter session: ssh g1-7 -N -L 8888 :127.0.0.1:8888 Now your are ready to connect to your notebook on your local browser, you just need the url suggested by Jupyter when you started your session. For example: http://127.0.0.1:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 . When you open this link in your browser, you should see the familiar Jupyter home screen: You can verify that this notebook is running on the remote host and within the conda environment as follows: First Time Setup If you have not used Jupyter on the cluster before, you will need to initialize your config files and profile. 1 This will generate new configuration files in ~/.jupyter and ~/.ipython for you. jupyter notebook --generate-config ipython profile create There is an issue that affects all Jupyter sessions where Jupyter and Ipython data directories reside on NFS mounts (specifically any of your research volumes or your HPC home directory). By default, Jupyter uses your home directory to store a couple of SQLite databases. However, due to an issue with file locking, SQLite is known to misbehave on some NFS mounts (see here ). How the problem presents itself: Your Jupyter sessions will simply hang and become unresponsive, as it will be stuck waiting for the SQLite databases\u2019 files to be locked, which is not supported over NFS home directories. How to resolve the issue You will need to tell Jupyter and Ipython to use memory to store the database information instead of creating a file in your home directory. This can be done by editing the configuration files for Jupyter and Ipython. (Which you may have just created by following the instructions above). # Edit Jupyter configuration vi ~/.jupyter/jupyter_notebook_config.py # Change following line # c.NotebookNotary.db_file = '' # To: c.NotebookNotary.db_file = ':memory:' # Edit IPython configuration vi ~/.ipython/profile_default/ipython_config.py # Change following two lines # c.HistoryManager.hist_file='' # c.HistoryAccessor.hist_file='' # To: c.HistoryManager.hist_file = ':memory:' c.HistoryAccessor.hist_file = ':memory:' This section is based on https://docs.hpc.sussex.ac.uk/apollo2/jupyter.html#first-time-setup-and-nfs-issue \u21a9","title":"Jupyter Notebooks"},{"location":"jupyter/#jupyter-notebook-workflow","text":"The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Visit Jupyter.org for more information.","title":"Jupyter Notebook Workflow"},{"location":"jupyter/#starting-jupyter-on-the-server","text":"To work with Jupyter notebooks on the cluster, a typical workflow is as follows: First, login to a node of the cluster where you'd like to run Jupyter on. # For example: ssh g1-7 Then, create a conda environment with your project dependencies and Jupyter. conda create -n myproject -c conda-forge python = 3 .8 pandas notebook conda activate myproject Start Jupyter instance on the cluster that listens on the networking interface at port 8888. If you have used Jupyter before, the output of this command should look similar to what you see on your local machine. $ jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser [I 06:30:19.290 NotebookApp] Serving notebooks from local directory: /homes/jan [I 06:30:19.290 NotebookApp] Jupyter Notebook 6.4.5 is running at: [I 06:30:19.290 NotebookApp] http://g1-7.ikim.uk-essen.de:8888/ ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 [I 06:30:19.290 NotebookApp] or http://127.0.0.1:8888/ ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 [I 06:30:19.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 06:30:19.296 NotebookApp] To access the notebook, open this file in a browser: file:///homes/jan/.local/share/jupyter/runtime/nbserver-426528-open.html Or copy and paste one of these URLs: http://g1-7.ikim.uk-essen.de:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 or http://127.0.0.1:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 Pay attention to the token in the output. In this case: d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 . This is the password which will allow you to access to the Jupyter session once you have connected your local machine to the remote Jupyter Session.","title":"Starting Jupyter on the Server"},{"location":"jupyter/#connecting-to-the-remote-jupyter","text":"Finally, run the following command on your local machine, to setup an SSH tunnel that connects to the remote Jupyter session: ssh g1-7 -N -L 8888 :127.0.0.1:8888 Now your are ready to connect to your notebook on your local browser, you just need the url suggested by Jupyter when you started your session. For example: http://127.0.0.1:8889/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9 . When you open this link in your browser, you should see the familiar Jupyter home screen: You can verify that this notebook is running on the remote host and within the conda environment as follows:","title":"Connecting to the remote Jupyter"},{"location":"jupyter/#first-time-setup","text":"If you have not used Jupyter on the cluster before, you will need to initialize your config files and profile. 1 This will generate new configuration files in ~/.jupyter and ~/.ipython for you. jupyter notebook --generate-config ipython profile create There is an issue that affects all Jupyter sessions where Jupyter and Ipython data directories reside on NFS mounts (specifically any of your research volumes or your HPC home directory). By default, Jupyter uses your home directory to store a couple of SQLite databases. However, due to an issue with file locking, SQLite is known to misbehave on some NFS mounts (see here ). How the problem presents itself: Your Jupyter sessions will simply hang and become unresponsive, as it will be stuck waiting for the SQLite databases\u2019 files to be locked, which is not supported over NFS home directories. How to resolve the issue You will need to tell Jupyter and Ipython to use memory to store the database information instead of creating a file in your home directory. This can be done by editing the configuration files for Jupyter and Ipython. (Which you may have just created by following the instructions above). # Edit Jupyter configuration vi ~/.jupyter/jupyter_notebook_config.py # Change following line # c.NotebookNotary.db_file = '' # To: c.NotebookNotary.db_file = ':memory:' # Edit IPython configuration vi ~/.ipython/profile_default/ipython_config.py # Change following two lines # c.HistoryManager.hist_file='' # c.HistoryAccessor.hist_file='' # To: c.HistoryManager.hist_file = ':memory:' c.HistoryAccessor.hist_file = ':memory:' This section is based on https://docs.hpc.sussex.ac.uk/apollo2/jupyter.html#first-time-setup-and-nfs-issue \u21a9","title":"First Time Setup"}]}